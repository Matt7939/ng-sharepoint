{"version":3,"file":"build/out.js","sources":["src/initialize.js","src/utils.js","src/cache.js","src/configuration.js","src/main.js"],"names":[],"mappings":"AAAA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CChCA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CClFA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;CC3CA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA,CACA;CCxCA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA,CACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA;AACA;AACA;AACA;AACA,CACA;AACA;AACA,CACA;AACA,CACA","sourcesContent":["/*global _, db, angular */\n\nvar _cache, _config, _utils, _debounce, CONST;\n\n// When caching is enabled, this variable stores references to queries to limit how fast a call can occur\n_debounce = {};\n\n/** Define the ngSharePoint module and SharePoint service\n *\n * An optional SP_CONFIG variable can be set to pass configuration options to the service:\n *\n *\n */\nangular\n\n\t.module('ngSharePoint', [])\n\n\t.factory('SharePoint',\n\n             ['$http',\n              'SP_CONFIG',\n\n              function ($http, SP_CONFIG) {\n\n\t              configuration(SP_CONFIG);\n\n\t              cache();\n\n\t              return main.apply(this, arguments);\n\n              }\n             ]);\n","/*global _utils, angular */\n\n\n_utils = {\n\t/**\n\t * Generate a timestamp offset from 1 Jan 2014 (EPOCH was too large and causing SP to throw a 500 error) :-/\n\t *\n\t * @returns {number} timestamp\n\t */\n\t'getTimeStamp': function () {\n\t\treturn Math.floor(new Date().getTime() / 1000 - CONST.EPOCH_OFFSET);\n\t},\n\n\t/**\n\t * Performs object cleanup prior to sending to SharePoint to prevent 500 errors\n\t *\n\t * @param scope\n\t * @returns {*}\n\t */\n\t'beforeSend': function (scope) {\n\n\t\tvar scopeClone = angular.copy(scope);\n\n\t\t// Empty the debounce list to prevent etag issues if the user is a really fast clicker!\n\t\t_debounce = {};\n\n\t\t// Add the timestamp if this is a cached request\n\t\tif (scopeClone.cache) {\n\t\t\tscopeClone.Timestamp = _utils.getTimeStamp();\n\t\t}\n\n\t\t// Remove non-model properties to prevent needless transmission/SP errors\n\t\tdelete scopeClone.__metadata;\n\t\tdelete scopeClone.callback;\n\t\tdelete scopeClone.cache;\n\n\t\t// JSON-encode any fields with the FIELD_JSON_TRAIL value\n\t\t_(scopeClone).each(function (s, field) {\n\n\t\t\tif (field.indexOf(CONST.FIELD_JSON_TRAIL) > 0) {\n\t\t\t\tscopeClone[field] = s !== null ? JSON.stringify(s) : '';\n\t\t\t}\n\n\t\t});\n\n\t\treturn scopeClone;\n\t},\n\n\t/**\n\t * Helper utility to convert SharePoint date strings to Date() objects with caching\n\t */\n\t'getDate': (function () {\n\n\t\tvar dCache = {};\n\n\t\treturn function (date) {\n\n\t\t\tif (date && !dCache[date]) {\n\n\t\t\t\tdCache[date] = Number(date.replace(/[^\\d.]/g, ''));\n\n\t\t\t}\n\n\t\t\treturn date ? new Date(dCache[date]) : null;\n\n\t\t};\n\n\t}()),\n\n\t/**\n\t * Creates a sanitized string for our cache key\n\t * @param options\n\t * @returns {*}\n\t */\n\t'cacheString': function (options) {\n\n\t\t// Remove all the junk from our JSON string of the model\n\t\treturn JSON.stringify(options).replace(/[^\\w]/gi, '') + _config.cacheVersion\n\n\t}\n\n};\n","function cache() {\n\n\t// If caching is enabled/available set it up\n\tif (!_config.noCache) {\n\n\t\t// Array to hold our callbacks while the cache DB is still loading, this will change to the cacheDB after init\n\t\t_cache = [];\n\n\t\tdb\n\n\t\t\t// Initialize the service\n\t\t\t.open(\n\t\t\t{\n\t\t\t\t'server' : 'angularSharePoint',\n\t\t\t\t'version': 1,\n\t\t\t\t'schema' : {\n\t\t\t\t\t'caches': {\n\t\t\t\t\t\t'keyPath': 'q'\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\n\t\t\t// Once the DB is loaded, try to run any cached callbacks and setup the cacheDB reference\n\t\t\t.done(function (instance) {\n\n\t\t\t\t      // Clone the _cache array to runners[]\n\t\t\t\t      var runners = _cache.slice(0);\n\n\t\t\t\t      // Remap _cache to instance (now acts as the cacheDB)\n\t\t\t\t      _cache = instance;\n\n\t\t\t\t      // Run all the callbacks async\n\t\t\t\t      while (runners.length) {\n\n\t\t\t\t\t      // Use shift() to reduce the array and pass a callback\n\t\t\t\t\t      setTimeout(runners.shift(), 25);\n\n\t\t\t\t      }\n\n\t\t\t      });\n\n\t}\n\n}","function configuration(SP_CONFIG) {\n\n\t// Constants for the service\n\tCONST = {\n\n\t\t// For caching, this is the initial timing offset (1 Jan 2014).  SP gives intermitten 500 errors if you use EPOCH\n\t\t'EPOCH_OFFSET'    : 1388552400,\n\n\t\t// The field name suffix for any JSON fields that will be automatically encoded/decoded by ng-sharepoint\n\t\t'FIELD_JSON_TRAIL': '_JSON',\n\n\t\t// For SP 2010 use 2.0 for 2013 it's 3.0.  This was added due to random 500 errors from a SP farm when this header wasn't sent (this is NOT required by the oData Spec)!\n\t\t'ODATA_VERSION'   : '2.0'\n\n\t};\n\n\t_config = _.defaults(\n\t\t// Load the SP_CONFIG variable if it exists\n\t\t\tSP_CONFIG || {},\n\n\t\t\t{\n\t\t\t\t// The URL for ListData.svc, default: /_vti_bin/ListData.svc\n\t\t\t\t'baseURL'     : '/_vti_bin/ListData.svc/',\n\n\t\t\t\t// The URL for loading user data, default: /_layouts/userdisp.aspx?Force=True\n\t\t\t\t'userURL'     : '/_layouts/userdisp.aspx?Force=True',\n\n\t\t\t\t// The URL for loading SP users, default: /_vti_bin/ListData.svc/UserInformationList\n\t\t\t\t'pplURL'      : '/_vti_bin/ListData.svc/UserInformationList',\n\n\t\t\t\t// Enable offline mode, doesn't check for changes if data is already cached\n\t\t\t\t'offline'     : false,\n\n\t\t\t\t// Override all caching options (automatic if db isn't loaded)\n\t\t\t\t'noCache'     : !db || false,\n\n\t\t\t\t// User-defined value.  Changing this will force all users to flush/re-validate all caches, useful for schema changes\n\t\t\t\t'cacheVersion': 1\n\t\t\t});\n\n}","function main($http, SP_CONFIG) {\n\n\treturn {\n\n\t\t/**\n\t\t *\n\t\t */\n\t\t'people': (function () {\n\n\t\t\t// This is the cache of our people queries\n\t\t\tvar _cachePeople = {};\n\n\t\t\treturn function (search, filter) {\n\n\t\t\t\t// Call the filter independently because it may be change while the SP data shouldn't\n\t\t\t\tvar execFilter = function (data) {\n\n\t\t\t\t\treturn filter ? _.filter(data, function (d) {\n\n\t\t\t\t\t\treturn filter(d);\n\n\t\t\t\t\t}) : data;\n\n\t\t\t\t};\n\n\t\t\t\t// If we've already done this search during the app's lifecycle, return it instead\n\t\t\t\tif (_cachePeople[search]) {\n\n\t\t\t\t\treturn {\n\t\t\t\t\t\t'then': function (callback) {\n\t\t\t\t\t\t\tcallback(execFilter(_cachePeople[search]));\n\t\t\t\t\t\t}\n\t\t\t\t\t};\n\n\t\t\t\t}\n\n\t\t\t\t// No cache existed so make the SP query\n\t\t\t\treturn $http(\n\t\t\t\t\t{\n\t\t\t\t\t\t'dataType': 'json',\n\t\t\t\t\t\t'method'  : 'GET',\n\t\t\t\t\t\t'cache'   : true,\n\t\t\t\t\t\t'url'     : _config.pplURL,\n\t\t\t\t\t\t'params'  : {\n\t\t\t\t\t\t\t'$select': 'Name,WorkEMail',\n\t\t\t\t\t\t\t'$filter': \"startswith(WorkEMail,'\" + search + \"')\",\n\t\t\t\t\t\t\t'$top'   : 10\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\t// Now convert to an array, store a copy in the cache and return results of execFilter()\n\t\t\t\t\t.then(function (response) {\n\n\t\t\t\t\t\t      var data = _cachePeople[search] = _.toArray(response.data.d);\n\t\t\t\t\t\t      return execFilter(data);\n\n\t\t\t\t\t      });\n\n\t\t\t};\n\n\t\t}()),\n\t\t'user'  : function (scope, sField) {\n\n\t\t\tvar scopeField = sField || 'user';\n\n\t\t\ttry {\n\n\t\t\t\tvar data = localStorage.getItem('SP_REST_USER');\n\n\t\t\t\tif (data) {\n\n\t\t\t\t\tdata = JSON.parse(data);\n\n\t\t\t\t\tif (new Date().getTime() - data.updated < 2592000000) {\n\n\t\t\t\t\t\tscope[scopeField] = data;\n\t\t\t\t\t\treturn;\n\n\t\t\t\t\t}\n\n\t\t\t\t}\n\n\t\t\t} catch (e) {\n\t\t\t}\n\n\t\t\treturn $http(\n\t\t\t\t{\n\t\t\t\t\t'method': 'GET',\n\t\t\t\t\t'cache' : true,\n\t\t\t\t\t'url'   : _config.userURL\n\t\t\t\t})\n\n\t\t\t\t.then(function (response) {\n\n\t\t\t\t\t      var data, html;\n\n\t\t\t\t\t      data = {\n\t\t\t\t\t\t      'id'     : parseInt(response.data.match(/_spuserid=(\\d+);/i)[1], 10),\n\t\t\t\t\t\t      'updated': new Date().getTime()\n\t\t\t\t\t      };\n\n\t\t\t\t\t      html = $(response.data.replace(/[ ]src=/g, ' data-src='));\n\n\t\t\t\t\t      html.find('#SPFieldText')\n\t\t\t\t\t\t      .each(function () {\n\n\t\t\t\t\t\t\t            var field1, field2;\n\n\t\t\t\t\t\t\t            field1 = this.innerHTML.match(/FieldName\\=\\\"(.*)\\\"/i)[1];\n\t\t\t\t\t\t\t            field2 = this.innerHTML.match(/FieldInternalName\\=\\\"(.*)\\\"/i)[1];\n\n\t\t\t\t\t\t\t            data[field1] = data[field2] = this.innerText.trim();\n\n\t\t\t\t\t\t            });\n\n\t\t\t\t\t      localStorage.SP_REST_USER = JSON.stringify(data);\n\n\t\t\t\t\t      scope[scopeField] = data;\n\n\t\t\t\t      });\n\n\t\t},\n\n\t\t'batch': function (collection) {\n\n\t\t\tvar map = [],\n\n\t\t\t    requests = _.map(collection, function (data) {\n\n\t\t\t\t    map.push(data);\n\n\t\t\t\t    return (data.__metadata.etag ?\n\n\t\t\t\t            [\n\t\t\t\t\t\t            'MERGE ' + data.__metadata.uri + ' HTTP/1.1',\n\t\t\t\t\t\t            'If-Match: ' + data.__metadata.etag\n\n\t\t\t\t            ] : ['POST ' + data.__metadata + ' HTTP/1.1'])\n\n\t\t\t\t\t    .concat(\n\t\t\t\t\t    [\n\t\t\t\t\t\t    'Content-Type: application/json;charset=utf-8',\n\t\t\t\t\t\t    'Accept: application/json',\n\t\t\t\t\t\t    '',\n\t\t\t\t\t\t    JSON.stringify(_utils.beforeSend(data))\n\t\t\t\t\t    ])\n\n\t\t\t\t\t    .join('\\n');\n\n\n\t\t\t    }),\n\n\t\t\t    // Generate a random string used for our multipart boundaries\n\t\t\t    seed = Math.random().toString(36).substring(2),\n\n\t\t\t    // Generate the boundary for this transaction set\n\t\t\t    boundary = 'b_' + seed,\n\n\t\t\t    // Generate the changeset that will separate each individual action\n\t\t\t    changeset = 'c_' + seed,\n\n\t\t\t    // The header that appears before each action(must have the extra linebreaks or SP will die)\n\t\t\t    header = [\n\t\t\t\t    '',\n\t\t\t\t    '--' + changeset,\n\t\t\t\t    'Content-Type: application/http',\n\t\t\t\t    'Content-Transfer-Encoding: binary',\n\t\t\t\t    '',\n\t\t\t\t    ''\n\t\t\t    ].join('\\n'),\n\n\t\t\t    // Create the body of the request with lots of linebreaks to make SP not sad.....\n\t\t\t    body = [\n\n\t\t\t\t    // Body start\n\t\t\t\t\t    '--' + boundary,\n\n\t\t\t\t    // Content type & changeset declaration\n\t\t\t\t\t    'Content-Type: multipart/mixed; boundary=' + changeset,\n\n\t\t\t\t    // Prepend a header to each request\n\t\t\t\t\t    header + requests.join(header),\n\n\t\t\t\t    // Another mandatory linebreak for SP\n\t\t\t\t\t    '',\n\n\t\t\t\t    // Close the changeset out\n\t\t\t\t\t    '--' + changeset + '--',\n\n\t\t\t\t    // Close the boundary as well\n\t\t\t\t\t    '--' + boundary + '--'\n\n\t\t\t    ].join('\\n');\n\n\t\t\t// Call $http against $batch with the mulitpart/mixed content type & our body\n\t\t\treturn $http(\n\t\t\t\t{\n\t\t\t\t\t'method' : 'POST',\n\t\t\t\t\t'url'    : _config.baseURL + '$batch',\n\t\t\t\t\t'headers': {\n\t\t\t\t\t\t'Content-Type'      : 'multipart/mixed; boundary=' + boundary,\n\t\t\t\t\t\t'DataServiceVersion': CONST.ODATA_VERSION\n\t\t\t\t\t},\n\t\t\t\t\tdata     : body\n\t\t\t\t})\n\n\t\t\t\t.then(function (response) {\n\n\t\t\t\t\t      var index = 0,\n\n\t\t\t\t\t          data = response.data,\n\n\t\t\t\t\t          processed = data.split(data.match(/boundary=([\\w-]+)/)[1]),\n\n\t\t\t\t\t          retVal = {\n\n\t\t\t\t\t\t          'success': true,\n\n\t\t\t\t\t\t          'transaction': {\n\t\t\t\t\t\t\t          'sent'    : response.config.data,\n\t\t\t\t\t\t\t          'received': data\n\t\t\t\t\t\t          }\n\n\t\t\t\t\t          };\n\n\t\t\t\t\t      processed = processed.slice(2, processed.length - 1);\n\n\t\t\t\t\t      _(processed).each(function (row) {\n\n\t\t\t\t\t\t      var callback = map[index++].callback,\n\n\t\t\t\t\t\t          etag = row.match(/ETag\\:\\s(.+)/i),\n\n\t\t\t\t\t\t          json;\n\n\t\t\t\t\t\t      if (retVal.success) {\n\n\t\t\t\t\t\t\t      retVal.success = (row.indexOf('HTTP/1.1 201') > 0) ||\n\t\t\t\t\t\t\t                       (row.indexOf('HTTP/1.1 204') > 0);\n\n\t\t\t\t\t\t\t      try {\n\t\t\t\t\t\t\t\t      json = JSON.parse(row.split(etag[0])[1].replace(/--$/, '')).d;\n\t\t\t\t\t\t\t      } catch (e) {\n\t\t\t\t\t\t\t\t      json = false;\n\t\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t\t      callback && callback(\n\t\t\t\t\t\t\t\t      {\n\t\t\t\t\t\t\t\t\t      'etag': etag[1],\n\t\t\t\t\t\t\t\t\t      'data': json\n\t\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t\t      }\n\n\t\t\t\t\t      });\n\n\t\t\t\t\t      return retVal;\n\n\t\t\t\t      });\n\n\n\t\t},\n\n\t\t/**\n\t\t * Create action\n\t\t *\n\t\t * Performs a CREATE with the given scope variable, The scope\n\t\t *\n\t\t * @param scope\n\t\t * @returns {*}\n\t\t */\n\t\t'create': function (scope) {\n\n\t\t\treturn $http(\n\t\t\t\t{\n\t\t\t\t\t'method' : 'POST',\n\t\t\t\t\t'url'    : _config.baseURL + scope.__metadata,\n\t\t\t\t\t'data'   : _utils.beforeSend(scope),\n\t\t\t\t\t'headers': {\n\t\t\t\t\t\t'DataServiceVersion': CONST.ODATA_VERSION\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t},\n\n\t\t'update': function (scope) {\n\n\t\t\treturn $http(\n\t\t\t\t{\n\t\t\t\t\t'method' : 'POST',\n\t\t\t\t\t'url'    : scope.__metadata.uri,\n\t\t\t\t\t'headers': {\n\t\t\t\t\t\t'If-Match'          : scope.__metadata.etag,\n\t\t\t\t\t\t'X-HTTP-Method'     : 'MERGE',\n\t\t\t\t\t\t'DataServiceVersion': CONST.ODATA_VERSION\n\t\t\t\t\t},\n\t\t\t\t\t'data'   : _utils.beforeSend(scope)\n\t\t\t\t});\n\n\t\t},\n\n\t\t'read': function (optOriginal) {\n\n\t\t\tvar getData, getCache, options;\n\n\t\t\toptions = angular.copy(optOriginal);\n\n\t\t\t// clear empty filters before we get started\n\t\t\tif (options.params && _.isEmpty(options.params.$filter)) {\n\t\t\t\tdelete options.params.$filter;\n\t\t\t}\n\n\t\t\t/**\n\t\t\t * getData $http wrapper, wraps the $http service with some SP-specific garbage\n\t\t\t *\n\t\t\t * @param opt Object\n\t\t\t * @returns {*|Promise}\n\t\t\t */\n\t\t\tgetData = function (opt) {\n\n\t\t\t\t// Join the params list if it is an array\n\t\t\t\t_(opt.params).each(function (param, key) {\n\n\t\t\t\t\t// Allows array params that we flatten to a string\n\t\t\t\t\tif (param instanceof Array) {\n\t\t\t\t\t\topt.params[key] = param.join(',');\n\t\t\t\t\t}\n\n\t\t\t\t\t// If this is a $select field and Id isn't specified, we'll need to add it for caching\n\t\t\t\t\tif (key === '$select' && param.indexOf('Id') < 0) {\n\t\t\t\t\t\topt.params.$select += ',Id';\n\t\t\t\t\t}\n\n\t\t\t\t});\n\n\t\t\t\treturn $http(\n\t\t\t\t\t{\n\t\t\t\t\t\t'dataType': 'json',\n\t\t\t\t\t\t'method'  : 'GET',\n\t\t\t\t\t\t'url'     : _config.baseURL + opt.source,\n\t\t\t\t\t\t'params'  : opt.params || null,\n\t\t\t\t\t\t'headers' : {\n\t\t\t\t\t\t\t'DataServiceVersion': CONST.ODATA_VERSION\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\t.then(function (response) {\n\n\t\t\t\t\t\t      var i = 0,\n\n\t\t\t\t\t\t          data = response.data.d.results || response.data.d,\n\n\t\t\t\t\t\t          decoder,\n\n\t\t\t\t\t\t          json = [],\n\n\t\t\t\t\t\t          dateWalk = function (item) {\n\t\t\t\t\t\t\t          _(item).each(function (el, index, parent) {\n\t\t\t\t\t\t\t\t          if (typeof el === 'object' || typeof el === 'array') {\n\n\t\t\t\t\t\t\t\t\t          return dateWalk(el);\n\n\t\t\t\t\t\t\t\t          } else {\n\n\t\t\t\t\t\t\t\t\t          if (typeof el === 'string' && el.indexOf('/Date(') > -1) {\n\n\t\t\t\t\t\t\t\t\t\t          parent[index] = _utils.getDate(el);\n\n\t\t\t\t\t\t\t\t\t          }\n\t\t\t\t\t\t\t\t          }\n\t\t\t\t\t\t\t          })\n\t\t\t\t\t\t          };\n\n\t\t\t\t\t\t      if (data.length) {\n\n\t\t\t\t\t\t\t      _(data[0]).each(function (d, f) {\n\n\t\t\t\t\t\t\t\t      if (f.indexOf(CONST.FIELD_JSON_TRAIL) > 1) {\n\t\t\t\t\t\t\t\t\t      json.push(f);\n\t\t\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t\t\t      decoder = function (v) {\n\n\t\t\t\t\t\t\t\t      if (json.length) {\n\t\t\t\t\t\t\t\t\t      _(json).each(function (field) {\n\n\t\t\t\t\t\t\t\t\t\t      v[field] = JSON.parse(v[field]);\n\n\t\t\t\t\t\t\t\t\t      });\n\t\t\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t\t\t      dateWalk(v);\n\n\t\t\t\t\t\t\t\t      return v;\n\n\t\t\t\t\t\t\t      };\n\n\t\t\t\t\t\t\t      return _.reduce(data, function (o, v) {\n\t\t\t\t\t\t\t\t      o[v.Id || i++] = decoder(v);\n\t\t\t\t\t\t\t\t      return o;\n\t\t\t\t\t\t\t      }, {});\n\n\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t      return data;\n\n\t\t\t\t\t      });\n\n\t\t\t};\n\n\t\t\t/**\n\t\t\t * getCache custom cache resolver/awesomeness generator\n\t\t\t * This will attempt to read indexerdb for any previously cached data and merge\n\t\t\t * updates with the cache.\n\t\t\t *\n\t\t\t * YOU MUST HAVE A SP FIELD NUMBER FIELD NAMED \"Timestamp\" FOR THIS TO WORK\n\t\t\t *\n\t\t\t * The Modified field WOULD have been perfect if SP oData requests filtered times properly :-/\n\t\t\t *\n\t\t\t * @param callback\n\t\t\t */\n\t\t\tgetCache = function (callback) {\n\n\t\t\t\t// Load the cached data, if it doesn't actually exist we'll deal with it later on\n\t\t\t\tvar runner = function () {\n\n\t\t\t\t\t// Create a cache key based on the model\n\t\t\t\t\tvar cacheString = _utils.cacheString(options);\n\n\t\t\t\t\t_cache.caches.get(cacheString).done(function (cachedData, opts, hasCache, oldStamp) {\n\n\t\t\t\t\t\tcachedData = cachedData || {'json': {}, 'time': false};\n\n\t\t\t\t\t\t// Offline enabled and the item exists, just return it without checking SP\n\t\t\t\t\t\tif (_config.offline && cachedData.time) {\n\n\t\t\t\t\t\t\tcallback(cachedData.json);\n\t\t\t\t\t\t\treturn;\n\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Really make this a boolean\n\t\t\t\t\t\thasCache = !!cachedData.time;\n\n\t\t\t\t\t\t// Save a copy of the old timestamp\n\t\t\t\t\t\toldStamp = cachedData.time;\n\n\t\t\t\t\t\t// Set a new timestamp before our network call (so we don't miss anything)\n\t\t\t\t\t\tcachedData.time = _utils.getTimeStamp();\n\n\t\t\t\t\t\t// If we already have cached data we need to add the timestamp to the filter\n\t\t\t\t\t\tif (hasCache) {\n\n\t\t\t\t\t\t\t// This is a messy comparison to see if we're under the debounce threshold\n\t\t\t\t\t\t\tif (_debounce[cacheString] &&\n\n\t\t\t\t\t\t\t    cachedData.time -\n\t\t\t\t\t\t\t    _debounce[cacheString] <\n\t\t\t\t\t\t\t    (options.debounce || 15)\n\n\t\t\t\t\t\t\t\t) {\n\n\t\t\t\t\t\t\t\tcallback(cachedData.json);\n\t\t\t\t\t\t\t\treturn;\n\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// Lazy man's deep object clone\n\t\t\t\t\t\t\topts = JSON.parse(JSON.stringify(options));\n\n\t\t\t\t\t\t\t// Start the filter with the timestamp--just in case SP is being dumb (optimization)\n\t\t\t\t\t\t\topts.params.$filter = '(Timestamp gt ' + oldStamp + ')' +\n\n\t\t\t\t\t\t\t                      (opts.params.$filter ?\n\t\t\t\t\t\t\t                       ' and ' + opts.params.$filter : '');\n\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// Add the last cachedData.time variable to our _debounce array\n\t\t\t\t\t\t_debounce[cacheString] = cachedData.time;\n\n\t\t\t\t\t\t// Call getData() with the custom opts or options as applicable\n\t\t\t\t\t\tgetData(opts || options)\n\n\t\t\t\t\t\t\t.then(function (data) {\n\n\t\t\t\t\t\t\t\t      // There was some data so we can add that to our cache and update everything\n\t\t\t\t\t\t\t\t      if (!_.isEmpty(data)) {\n\n\t\t\t\t\t\t\t\t\t      // Merge our updates with the cache\n\t\t\t\t\t\t\t\t\t      _(data).each(function (row, key) {\n\t\t\t\t\t\t\t\t\t\t      cachedData.json[key] = row;\n\t\t\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t\t\t\t\t      // Fire & forget--just add this and keep going\n\t\t\t\t\t\t\t\t\t      _cache.caches.update(\n\t\t\t\t\t\t\t\t\t\t      {\n\t\t\t\t\t\t\t\t\t\t\t      'item': cachedData,\n\t\t\t\t\t\t\t\t\t\t\t      'key' : cacheString\n\t\t\t\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t\t\t\t\t      if (hasCache) {\n\n\t\t\t\t\t\t\t\t\t\t      // Add an updated=true property to our response\n\t\t\t\t\t\t\t\t\t\t      _(data).each(function (row, key) {\n\t\t\t\t\t\t\t\t\t\t\t      cachedData.json[key].updated = true;\n\t\t\t\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t\t\t      }\n\n\t\t\t\t\t\t\t\t      // All done, do the callback\n\t\t\t\t\t\t\t\t      callback(cachedData.json);\n\n\t\t\t\t\t\t\t      });\n\n\t\t\t\t\t});\n\n\t\t\t\t};\n\n\t\t\t\t// Que up the requests if the _cache DB isn't loaded yet\n\t\t\t\tif (typeof _cache.length === 'number') {\n\t\t\t\t\t_cache.push(runner);\n\t\t\t\t} else {\n\t\t\t\t\trunner();\n\t\t\t\t}\n\n\t\t\t};\n\n\t\t\t// If caching is disabled for the service, then override the request\n\t\t\tif (_config.noCache) {\n\t\t\t\toptions.cache = false;\n\t\t\t}\n\n\t\t\t// Return the getData or getCache promises\n\t\t\treturn !options.cache ?\n\n\t\t\t\t// Return getData()'s $http promises, no caching\n\t\t\t\t   getData(options) :\n\n\t\t\t\t// Return getCache()'s custom promises, caching is enabled\n\t\t\t\t   {\n\n\t\t\t\t\t   'then'   : getCache,\n\t\t\t\t\t   'catch'  : function () {\n\t\t\t\t\t   },\n\t\t\t\t\t   'finally': function () {\n\t\t\t\t\t   }\n\n\t\t\t\t   };\n\t\t}\n\n\t}\n\n};"]}